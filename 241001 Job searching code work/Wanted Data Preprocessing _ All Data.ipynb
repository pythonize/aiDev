{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580e6401",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642aefdf",
   "metadata": {},
   "source": [
    "주요 어휘 추출 전처리 후 페이지 추천 기능 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3f751",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511644ac",
   "metadata": {},
   "source": [
    "1. 전처리 코드 완성(디버깅 포함, 워드클라우드 생성)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb62937",
   "metadata": {},
   "source": [
    "※ 라이브러리 및 프레임워크 필요시 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567a976",
   "metadata": {},
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3fc79",
   "metadata": {},
   "source": [
    "!pip install JPype1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6da02",
   "metadata": {},
   "source": [
    "!pip install wordcloud matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec644e3",
   "metadata": {},
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51628283",
   "metadata": {},
   "source": [
    "!pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/wanted_crawling_all_data.csv\", index_col=None, \n",
    "                 parse_dates=['Title', 'URL'], encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트를 파일에서 읽어오기\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의 (명사 추출 및 불용어 제거, 영어 포함)\n",
    "def preprocess_text(text, okt):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 소문자로 변환 (영어에 유용)\n",
    "        text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "        # 명사 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 명사와 함께 영어 단어도 추출하기 위해 영어 필터링 추가\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)  # 영어 단어 추출\n",
    "        # 불용어 제거 및 단어 필터링\n",
    "        filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "        filtered_english = [word for word in english_words if word not in stopwords]\n",
    "        # 한국어 명사와 영어 단어를 결합하여 반환\n",
    "        return ' '.join(filtered_nouns + filtered_english)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 기반 중요 단어 추출 함수\n",
    "def get_important_words(column_text, n=50):\n",
    "    vectorizer = TfidfVectorizer(max_features=n, max_df=0.85, min_df=1) \n",
    "    X = vectorizer.fit_transform(column_text)\n",
    "    if X.shape[0] == 0:  # 문서가 없는 경우\n",
    "        return []\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = X.toarray().sum(axis=0)\n",
    "    word_score_pairs = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
    "    return word_score_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63756b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열에 대해 전처리 및 TF-IDF 기반 중요 단어 추출 (URL 칼럼 제외)\n",
    "important_words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b73dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    # 해당 칼럼은 처리하지 않음\n",
    "    if column in ['Title', 'Company', 'Career', 'Deadline', 'Location', 'Duty', 'URL']:\n",
    "        continue\n",
    "    if df[column].dtype == 'object':  # 문자열 데이터에 대해서만 처리\n",
    "        # 전처리 및 명사 추출\n",
    "        df[column] = df[column].apply(preprocess_text)\n",
    "        column_text = df[column].dropna().tolist()  # NaN 값 제거 및 리스트로 변환\n",
    "        \n",
    "        # 텍스트 샘플 출력 (디버깅용)\n",
    "        print(f\"Column: {column}\")\n",
    "        print(\"Sample Texts:\")\n",
    "        print(column_text[:5])  # 상위 5개 텍스트 샘플 출력\n",
    "        \n",
    "        if len(column_text) > 0:  # 데이터가 있는 경우에만 처리\n",
    "            # TF-IDF 기반 중요 단어 추출\n",
    "            important_words = get_important_words(column_text, n=50)\n",
    "            if important_words:  # 중요 단어가 추출된 경우에만 처리\n",
    "                important_words_dict[column] = important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 칼럼별 중요 단어를 CSV 파일로 저장\n",
    "for column, words in important_words_dict.items():\n",
    "    df_words = pd.DataFrame(words, columns=['Word', 'Score'])\n",
    "    df_words.to_csv(f'{column}_important_words.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 텍스트 확인 (디버깅용)\n",
    "for column in df.columns:\n",
    "    if column in important_words_dict:\n",
    "        print(f\"Column: {column}\")\n",
    "        print(df[column].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터프레임 저장\n",
    "df.to_csv('preprocessed_data_all.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c15748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 워드클라우드 생성 및 저장 함수\n",
    "def create_wordcloud(words, column_name):\n",
    "    word_freq = dict(words)  # 중요 단어와 점수를 딕셔너리 형태로 변환\n",
    "    wordcloud = WordCloud(font_path='C:/Windows/Fonts/malgun.ttf',  # 한글 폰트 경로\n",
    "                          width=800, height=400, \n",
    "                          background_color='white').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for {column_name}')\n",
    "    plt.savefig(f'{column_name}_wordcloud.png', format='png')\n",
    "    plt.show()\n",
    "\n",
    "# 각 칼럼별 워드클라우드 생성 및 저장\n",
    "for column, words in important_words_dict.items():\n",
    "    create_wordcloud(words, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f1887",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151e907",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfba742",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f448ce",
   "metadata": {},
   "source": [
    "2. 데이터 전처리 후 추천기능 적용(통합검색 추천)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce160a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/wanted_crawling_all_data.csv\", index_col=None, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트를 파일에서 읽어오기\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# 전처리 함수 정의 (명사 추출 및 불용어 제거, 영어 포함)\n",
    "def preprocess_text(text, okt):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 소문자로 변환 (영어에 유용)\n",
    "        text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "        # 명사 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 명사와 함께 영어 단어도 추출하기 위해 영어 필터링 추가\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)  # 영어 단어 추출\n",
    "        # 불용어 제거 및 단어 필터링\n",
    "        filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "        filtered_english = [word for word in english_words if word not in stopwords]\n",
    "        # 한국어 명사와 영어 단어를 결합하여 반환\n",
    "        return ' '.join(filtered_nouns + filtered_english)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87134f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자의 조건을 입력받아 유사한 채용 공고 추천하는 함수\n",
    "def recommend_jobs(user_query, df, okt, top_n=5):\n",
    "    # NaN 값을 빈 문자열로 대체\n",
    "    df.fillna('', inplace=True)\n",
    "    # 여러 칼럼을 전처리 (Title, Company, 주요 업무, 기술 스택 등 필요한 칼럼 추가)\n",
    "    df['preprocessed_title'] = df['Title']\n",
    "    df['preprocessed_company'] = df['Company']\n",
    "    df['preprocessed_career'] = df['Career']\n",
    "    df['preprocessed_work'] = df['Work'].apply(lambda x: preprocess_text(x, okt))\n",
    "    df['preprocessed_qualification'] = df['Qualification'].apply(lambda x: preprocess_text(x, okt))\n",
    "    df['preprocessed_addition'] = df['Addition'].apply(lambda x: preprocess_text(x, okt))\n",
    "    df['preprocessed_welfare'] = df['Welfare'].apply(lambda x: preprocess_text(x, okt))\n",
    "    df['preprocessed_skill'] = df['Skill']\n",
    "    df['preprocessed_tag'] = df['Tag']\n",
    "    df['preprocessed_deadline'] = df['Deadline']\n",
    "    df['preprocessed_location'] = df['Location']\n",
    "    df['preprocessed_duty'] = df['Duty']\n",
    "    \n",
    "    # 사용자가 입력한 질의를 전처리\n",
    "    query_preprocessed = preprocess_text(user_query, okt)\n",
    "\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # 여러 칼럼을 하나의 텍스트로 결합 (공백을 추가하여 결합)\n",
    "    df['combined_text'] = df['preprocessed_title'] + ' ' + df['preprocessed_company'] + ' ' + df['preprocessed_career'] + ' ' + df['preprocessed_work'] + ' ' + df['preprocessed_qualification'] + ' ' + df['preprocessed_addition'] + ' ' + df['preprocessed_welfare'] + ' ' + df['preprocessed_skill'] + ' ' + df['preprocessed_tag'] + ' ' + df['preprocessed_deadline'] + ' ' + df['preprocessed_location'] + ' ' + df['preprocessed_duty']\n",
    "    combined_matrix = vectorizer.fit_transform(df['combined_text'])\n",
    "    \n",
    "    # 사용자의 질의를 벡터화\n",
    "    query_vector = vectorizer.transform([query_preprocessed])\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity_scores = cosine_similarity(query_vector, combined_matrix).flatten()\n",
    "\n",
    "    # 유사도 순으로 상위 N개의 인덱스 추출\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "\n",
    "    # 상위 N개의 회사, 제목, URL을 추천\n",
    "    recommendations = df[['Title', 'Company', 'URL']].iloc[top_indices]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 선택한 조건\n",
    "user_query = \"서울에 있는 신입 뽑는 IT 기업 추천해줘\"\n",
    "\n",
    "# 채용 공고 추천\n",
    "recommended_jobs = recommend_jobs(user_query, df, okt, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추천 결과 출력\n",
    "print(recommended_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509351c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499cbeb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab62b2f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804eae0",
   "metadata": {},
   "source": [
    "3. 문답 형식에 따른 채용공고 추천 코드(조건에 따른 검색 추천)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb4e2c",
   "metadata": {},
   "source": [
    " 1) 구글 맵스를 활용한 근무지 위치 정확한 주소로 변경(주소 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44553ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/wanted_crawling_all_data.csv\", index_col=None, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps_key = 'AIzaSyBWUgRUl_18YTfLl4hXjNPfNBYRh-HwF40'\n",
    "gmaps = googlemaps.Client(key = gmaps_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76083b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_location(location):\n",
    "    tmp = gmaps.geocode(location, language='ko')\n",
    "    if tmp:\n",
    "        return tmp[0].get('formatted_address')\n",
    "    return location\n",
    "\n",
    "# tqdm으로 진행 상황 표시\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # tqdm을 사용하여 진행 상황 표시\n",
    "    df['Location'] = list(tqdm(executor.map(fetch_location, df['Location']), total=len(df['Location'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7e8dc",
   "metadata": {},
   "source": [
    "# 'Location' 칼럼 정확한 주소로 변경\n",
    "for i in range(len(df['Location'])):\n",
    "    location = df['Location'][i]\n",
    "    \n",
    "    # Google Maps 주소 변환\n",
    "    tmp = gmaps.geocode(location, language='ko')\n",
    "    \n",
    "    #'formatted_address'로 Location 값 업데이트\n",
    "    if tmp:  # tmp 리스트가 존재할 시\n",
    "        df['Location'][i] = tmp[0].get('formatted_address')\n",
    "\n",
    "# 결과 확인\n",
    "print(df['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터프레임 저장\n",
    "df.to_csv('preprocessed_locationdata_all.csv', index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a326e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac692111",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc517e82",
   "metadata": {},
   "source": [
    "2) 조건에 따른 검색기능(주소 전처리 후 조건 검색 기능 추가)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bdf45f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/preprocessed_locationdata_all.csv\", index_col=None, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트를 파일에서 읽어오기\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# 전처리 함수 정의 (명사 추출 및 불용어 제거, 영어 포함)\n",
    "def preprocess_text(text, okt):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 소문자로 변환\n",
    "        text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "        # 명사 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 영어 단어 추출\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "        # 불용어 제거 및 단어 필터링\n",
    "        filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "        filtered_english = [word for word in english_words if word not in stopwords]\n",
    "        # 한국어 명사와 영어 단어를 결합하여 반환\n",
    "        return ' '.join(filtered_nouns + filtered_english)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 검색어 입력\n",
    "def ask_question(question):\n",
    "    response = input(question + \" \")\n",
    "    return response\n",
    "\n",
    "# 조건에 따른 필터링\n",
    "def filter_jobs_by_criteria(df, location=None, duty=None, career=None):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # 조건에 따라 데이터 필터링\n",
    "    if location and not pd.isna(location):\n",
    "        filtered_df = filtered_df[filtered_df['Location'].str.strip().str.contains(location.strip(), case=False, na=False)]\n",
    "    if duty and not pd.isna(duty):\n",
    "        filtered_df = filtered_df[filtered_df['Duty'].str.strip().str.contains(duty.strip(), case=False, na=False)]\n",
    "    if career and not pd.isna(career):\n",
    "        filtered_df = filtered_df[filtered_df['Career'].str.strip().str.contains(career.strip(), case=False, na=False)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# 유사 채용 공고 추천 함수 (문답에 따른 범위 좁히기)\n",
    "def recommend_interactively(df, okt):\n",
    "    # 사용자의 선택 조건을 순차적으로 입력\n",
    "    location = ask_question(\"선호하는 지역을 말씀해주세요:\")\n",
    "    duty = ask_question(\"선호하는 직무를 말씀해주세요:\")\n",
    "    career = ask_question(\"경력에 대한 조건을 말씀해주세요:\")\n",
    "\n",
    "    # 필터링된 데이터프레임\n",
    "    filtered_df = filter_jobs_by_criteria(df, location, duty, career)\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"해당 조건에 맞는 채용 공고가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 전처리 (필요한 칼럼에 대한 전처리 수행)\n",
    "    filtered_df['preprocessed_work'] = filtered_df['Work'].apply(lambda x: preprocess_text(x, okt))\n",
    "    filtered_df['preprocessed_qualification'] = filtered_df['Qualification'].apply(lambda x: preprocess_text(x, okt))\n",
    "    filtered_df['preprocessed_addition'] = filtered_df['Addition'].apply(lambda x: preprocess_text(x, okt))\n",
    "    filtered_df['preprocessed_welfare'] = filtered_df['Welfare'].apply(lambda x: preprocess_text(x, okt))\n",
    "\n",
    "    # 사용자 질의를 입력받고 전처리\n",
    "    user_query = ask_question(\"검색하려는 키워드를 입력해주세요:\")\n",
    "    query_preprocessed = preprocess_text(user_query, okt)\n",
    "\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # 여러 칼럼을 결합하여 벡터화\n",
    "    filtered_df['combined_text'] = (\n",
    "        filtered_df['Title'].fillna('') + ' ' +\n",
    "        filtered_df['Company'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_work'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_qualification'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_addition'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_welfare'].fillna('') + ' ' +\n",
    "        filtered_df['Skill'].fillna('') + ' ' +\n",
    "        filtered_df['Tag'].fillna('') + ' ' +\n",
    "        filtered_df['Deadline'].fillna('') + ' ' +\n",
    "        filtered_df['Location'].fillna('') + ' ' +\n",
    "        filtered_df['Duty'].fillna('')\n",
    "    )\n",
    "\n",
    "    # TfidfVectorizer를 적용\n",
    "    combined_matrix = vectorizer.fit_transform(filtered_df['combined_text'])\n",
    "\n",
    "    # 사용자 질의를 벡터화\n",
    "    query_vector = vectorizer.transform([query_preprocessed])\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity_scores = cosine_similarity(query_vector, combined_matrix).flatten()\n",
    "\n",
    "    # 유사도 순으로 상위 5개의 인덱스 추출\n",
    "    top_indices = similarity_scores.argsort()[-5:][::-1]\n",
    "\n",
    "    # 상위 5개의 회사, 제목, URL을 추천\n",
    "    recommendations = filtered_df[['Title', 'Company', 'URL']].iloc[top_indices]\n",
    "    \n",
    "    print(\"추천 결과:\")\n",
    "    print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd53473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채용 공고 추천 시스템 실행\n",
    "recommend_interactively(df, okt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bac970",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df4cdd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076db68",
   "metadata": {},
   "source": [
    "3) 조건에 따른 채용공고 추천(오타 가능) - 정확도 떨어짐(폐기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d716b9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ddbbd",
   "metadata": {},
   "source": [
    "!pip install fuzzywuzzy[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341506bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/preprocessed_locationdata_all.csv\", index_col=None, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트를 파일에서 읽어오기\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의 (명사 추출 및 불용어 제거, 영어 포함)\n",
    "def preprocess_text(text, okt):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 소문자로 변환\n",
    "        text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "        # 명사 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 영어 단어 추출\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "        # 불용어 제거 및 단어 필터링\n",
    "        filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "        filtered_english = [word for word in english_words if word not in stopwords]\n",
    "        # 한국어 명사와 영어 단어를 결합하여 반환\n",
    "        return ' '.join(filtered_nouns + filtered_english)\n",
    "    return ''\n",
    "\n",
    "# 유사한 텍스트 찾기 함수 (fuzzy 매칭)\n",
    "def fuzzy_match(input_text, choices, threshold=70):\n",
    "    if not input_text or pd.isna(input_text):\n",
    "        return None\n",
    "    # 입력 텍스트와 choices 리스트에서 가장 유사한 텍스트 추출\n",
    "    best_match = process.extractOne(input_text.strip(), choices, score_cutoff=threshold)\n",
    "    if best_match:\n",
    "        return best_match[0]  # best_match는 (matched_text, score)의 튜플로 반환됨\n",
    "    return None\n",
    "\n",
    "# 조건에 따른 필터링 (fuzzy 매칭 적용)\n",
    "def filter_jobs_by_criteria(df, location=None, duty=None, career=None):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Location에 따른 fuzzy 필터링\n",
    "    if location and not pd.isna(location):\n",
    "        location_choices = filtered_df['Location'].dropna().unique().tolist()\n",
    "        matched_location = fuzzy_match(location, location_choices)\n",
    "        if matched_location:\n",
    "            filtered_df = filtered_df[filtered_df['Location'].str.contains(matched_location, case=False, na=False)]\n",
    "\n",
    "    # Duty에 따른 fuzzy 필터링\n",
    "    if duty and not pd.isna(duty):\n",
    "        duty_choices = filtered_df['Duty'].dropna().unique().tolist()\n",
    "        matched_duty = fuzzy_match(duty, duty_choices)\n",
    "        if matched_duty:\n",
    "            filtered_df = filtered_df[filtered_df['Duty'].str.contains(matched_duty, case=False, na=False)]\n",
    "\n",
    "    # Career에 따른 fuzzy 필터링\n",
    "    if career and not pd.isna(career):\n",
    "        career_choices = filtered_df['Career'].dropna().unique().tolist()\n",
    "        matched_career = fuzzy_match(career, career_choices)\n",
    "        if matched_career:\n",
    "            filtered_df = filtered_df[filtered_df['Career'].str.contains(matched_career, case=False, na=False)]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62619318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 검색어 입력\n",
    "def ask_question(question):\n",
    "    response = input(question + \" \")\n",
    "    return response\n",
    "\n",
    "# 유사 채용 공고 추천 함수 (문답에 따른 범위 좁히기)\n",
    "def recommend_interactively(df, okt):\n",
    "    # 사용자의 선택 조건을 순차적으로 입력\n",
    "    location = ask_question(\"선호하는 지역을 말씀해주세요:\")\n",
    "    duty = ask_question(\"선호하는 직무를 말씀해주세요:\")\n",
    "    career = ask_question(\"경력에 대한 조건을 말씀해주세요:\")\n",
    "\n",
    "    # 필터링된 데이터프레임\n",
    "    filtered_df = filter_jobs_by_criteria(df, location, duty, career)\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"해당 조건에 맞는 채용 공고가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 전처리 (필요한 칼럼에 대한 전처리 수행)\n",
    "    filtered_df['preprocessed_work'] = filtered_df['Work'].apply(lambda x: preprocess_text(x, okt))\n",
    "    filtered_df['preprocessed_qualification'] = filtered_df['Qualification'].apply(lambda x: preprocess_text(x, okt))\n",
    "    filtered_df['preprocessed_addition'] = filtered_df['Addition'].apply(lambda x: preprocess_text(x, okt))\n",
    "    filtered_df['preprocessed_welfare'] = filtered_df['Welfare'].apply(lambda x: preprocess_text(x, okt))\n",
    "\n",
    "    # 사용자 질의를 입력받고 전처리 없이 저장\n",
    "    user_query = ask_question(\"검색하려는 키워드를 입력해주세요:\")\n",
    "    \n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # 여러 칼럼을 결합하여 벡터화\n",
    "    filtered_df['combined_text'] = (\n",
    "        filtered_df['Title'].fillna('') + ' ' +\n",
    "        filtered_df['Company'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_work'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_qualification'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_addition'].fillna('') + ' ' +\n",
    "        filtered_df['preprocessed_welfare'].fillna('') + ' ' +\n",
    "        filtered_df['Skill'].fillna('') + ' ' +\n",
    "        filtered_df['Tag'].fillna('') + ' ' +\n",
    "        filtered_df['Deadline'].fillna('') + ' ' +\n",
    "        filtered_df['Location'].fillna('') + ' ' +\n",
    "        filtered_df['Duty'].fillna('')\n",
    "    )\n",
    "\n",
    "    # TfidfVectorizer를 적용\n",
    "    combined_matrix = vectorizer.fit_transform(filtered_df['combined_text'])\n",
    "\n",
    "    # 사용자 질의를 벡터화\n",
    "    query_vector = vectorizer.transform([user_query])  # 전처리 없이 사용자 질의 사용\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity_scores = cosine_similarity(query_vector, combined_matrix).flatten()\n",
    "\n",
    "    # 유사도 순으로 상위 10개의 인덱스 추출\n",
    "    top_indices = similarity_scores.argsort()[-10:][::-1]\n",
    "\n",
    "    # 상위 10개의 회사, 제목, URL을 추천\n",
    "    recommendations = filtered_df[['Title', 'Company', 'URL', 'combined_text']].iloc[top_indices]\n",
    "\n",
    "    # fuzzy matching을 사용하여 유사한 키워드를 찾기\n",
    "    fuzzy_recommendations = []\n",
    "    for idx in top_indices:\n",
    "        row = recommendations.iloc[top_indices.tolist().index(idx)]  # 수정된 부분\n",
    "        score = fuzz.partial_ratio(user_query, row['combined_text'])  # 전처리 없이 사용자 질의 사용\n",
    "        combined_score = (similarity_scores[idx] + score) / 2  # 코사인 유사도와 fuzzy 점수 결합\n",
    "        fuzzy_recommendations.append((row['Title'], row['Company'], row['URL'], combined_score))\n",
    "\n",
    "    # 유사도 점수를 기준으로 상위 5개 추출\n",
    "    fuzzy_recommendations = sorted(fuzzy_recommendations, key=lambda x: x[3], reverse=True)[:5]\n",
    "\n",
    "    # 추천 결과 출력\n",
    "    print(\"추천 결과:\")\n",
    "    for title, company, url, score in fuzzy_recommendations:\n",
    "        print(f\"회사: {company}, 제목: {title}, URL: {url}, 유사도: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bdcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채용 공고 추천 시스템 실행\n",
    "recommend_interactively(df, Okt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff5a4b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff62c0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675129b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853489c",
   "metadata": {},
   "source": [
    " 4. 전처리 과정 분리 후 조건에 따른 검색(241001 화요일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2def854",
   "metadata": {},
   "source": [
    "1) 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abe3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48043eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/preprocessed_locationdata_all.csv\", index_col=None, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94f167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트를 파일에서 읽어오기\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ff8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a9434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의 (명사 추출 및 불용어 제거, 영어 포함)\n",
    "def preprocess_text(text, okt):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 소문자로 변환 (영어에 유용)\n",
    "        text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "        # 명사 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 명사와 함께 영어 단어도 추출하기 위해 영어 필터링 추가\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)  # 영어 단어 추출\n",
    "        # 불용어 제거 및 단어 필터링\n",
    "        filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "        filtered_english = [word for word in english_words if word not in stopwords]\n",
    "        # 한국어 명사와 영어 단어를 결합하여 반환\n",
    "        return ' '.join(filtered_nouns + filtered_english)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5766b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 기반 중요 단어 추출 함수\n",
    "def get_important_words(column_text, n=50):\n",
    "    vectorizer = TfidfVectorizer(max_features=n, max_df=0.85, min_df=1) \n",
    "    X = vectorizer.fit_transform(column_text)\n",
    "    if X.shape[0] == 0:  # 문서가 없는 경우\n",
    "        return []\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = X.toarray().sum(axis=0)\n",
    "    word_score_pairs = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
    "    return word_score_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae446d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열에 대해 전처리 및 TF-IDF 기반 중요 단어 추출 (URL 칼럼 제외)\n",
    "important_words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01aab323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Work\n",
      "Sample Texts:\n",
      "['주요 책임 글로벌 유저 대상 게임 플랫폼 개발 플랫폼 서비스 운영 지속 개선 게임 서비스 백오피스 기능 개발 도화주 환경 프로젝트 일정 집중 기간 존재 web', '합류 토스 레이스 오프라인 결제 시장 디지털 혁신 만들기 위해 매장 운영 문제 해결 레이스 초창 멤버 로서 오프라인 결제 시장 마주 도전 문제 처음 고민 개발 사장 대시보드 홈페이지 웹사이트 토스 결제 단말기 서비스 웹뷰 모든 플랫폼 기술 이용 사용자 향상 위해 토스 공통 라이브러리 제작 기여 windowsmacosandroidios tds', '프레임워크 개발 유지 보수 자사 솔루션 백오피스 비롯 마이크로 사이트 개발 유지 보수 활용 데이터 바인 렌더링 설계 구현 vuejs ui', '모바일 게임 마피아 서버 개발자 채팅 모바일 온라인 게임 서버 개발', '제휴 파트너 효과 테크 서비스 개발 성장 중인 트래블 월렛 차세대 외환 결제 백오피스 개발 퍼블릭 웹서비스 개발 운영 백오피스 사내 운영 개발 운영 travelwallet enterprise inapp webview']\n",
      "Column: Qualification\n",
      "Sample Texts:\n",
      "['필수 자격 요건 학력 전공 무관 개발 보유 유저 서비스 개발 보유 이해 지식 보유 라이브러리 프레임워크 활용 개발 관심 선호 자격 요건 개발 언어 로서 이해 언어 강화 위해 노력 환경 모바일 웹뷰 보유 기술 이해 렌더링 개발 배포 스크립트 작성 구축 운용 지식 협업 실무 보유 디자인 시스템 실무 보유 웹사이트 성능 최적화 분필 효과 커뮤니케이션 여러 유관 부서 외부 관계자 협업 협력 사고 통합 관점 목표 의식 주도 추진 문제 파악 분석 적극 선제 문제해결 책임감 결과물 품질 완성 기준 typescript html css nextjs react svelte vuejs javascript webview cef isr cicd monorepo figma', '분과 프레임워크 사용 개발 주도 문제 발견 분석 해결 이용 정적 타입 분석 react vue angular spa typescript javascript', '이해 프론트엔드 프레임워크 이해 개발 의사소통 협업 javascripttypescript vuejs reactjs saas', '자료구조 알고리즘 운영체제 네트워크 컴퓨터 학적 지식 언어 지식 설계 개발 포트폴리오 제출 필수 java kotlin jvm', '관련 개발 실력 보유 활용 어플리케이션 평소 사용 사용 커뮤니케이션 react typescript nextjs reacthookform reactquery eslint prettier linter git jira slack notion']\n",
      "Column: Addition\n",
      "Sample Texts:\n",
      "['조직 문화 자율 출퇴근 무제한 연차 휴가 스스로 관리 의견 교환 상호 신뢰 근거 수평 커뮤니케이션 직급 모든 구성원 호칭 존댓말 사용 장소 상황 개성 표현 자유 복장 사무 환경 몰입 최적 사무 공간 효율 진행 위해 직무 최적화 장비 소프트웨어 창의력 컨셉 휴게 공간 휴게 공간 구성원 신체 심리 건강 피트니스 센터 심리상담 서비스 운영 가족 생활 가족 탄생 축하 출산휴가 육아휴직 출산휴가 최대 급여 해외 출장 구성원 안전 해외 재난 구조 서비스 상조 서비스 종합 검진 년회 실손 보험 이벤트 문화 활동 컬처 데이 리더 구성원 수평 소통 실천 타운 미팅 구성원 상호 이해 랜덤 런치 네트워킹 프로그램 빅히트 사다리 사항 온보딩 프로그램 적용 수습 기간 개월 프로그램 빅히트 고유 온보딩 과정 하이브 인재 상과 직문 의미 열정 자율 신뢰 이해 돕기 위해 진행 프로그램 기간 급여 복리 후생 차등 최종 평가 결과 사내 기준 부합 경우 채용 취소 장애인 국가 보훈 대상자 관련 의거 우대 제출 자료 채용 프로세스 전반 허위 사실 발견 경우 채용 취소 채용 프로세스 서류 전형 제출 서류 담당자 검토 실무 면접 전형 실무 직책 수행 관련 인터뷰 진행 조직 합성 면접 전형 피플 구성원 빅히트 방식 방식 부합 상호 확인 인터뷰 리더 면접 전형 부문 리더 실무 조직 합성 종합 확인 처우 협의 최종 합격 모든 전형 합격 최종 합격 안내 이후 처우 입사 협의 진행 참고 사항 직무 성격 실무 과제 요청 테스트 진행 해당 채용 수시 채용 형식 모든 면접 일대일 다대 일로 진행 최대 가량 소요 win together win together', '이력서 작성 추천 플랫폼 관련 공통 서비스 공통 라이브러리 구현 운영 해당 진행 이유 상세 기술 고가용성 확장 시스템 설계 운영 설계 근거 관련 대해 상세 기술 대규모 실시간 트래픽 처리 시스템 설계 운영 설계 근거 관련 대해 상세 기술 서비스 운영 트러블 슈팅 성능 개선 구체 과정 기술 토스 레이스 사용 기술 상태 관리 패키지 매니저 빌드 테스트 react typescript nextjs reactquery recoil yarn berry pnpm webpack esbuild swc babel jest vitest cicd github actions', '이해 관심 사용 성능 최적화 관심 이해 테스트 자동화 단위 테스트 라이브러리 모듈 rwaresponsive web app pwaprogressive web figma ux tool ui', '게임 웹앱 온라인 서비스 백엔드 개발 용량 부하 시스템 설계 운영 리눅스 운영 서버 배포 오픈소스 개발 참여 이슈 등록 프로젝트 진행 에러 사항 해결 과정 설명 서버 개발 프레임워크 사용 pr java netty', '배포 클라우드 배포 환경 이해 프론트엔드 개발 방법 이해 개발 디자인 시스템 재사용 컴포넌트 개발 이해 기술 토론 공유 긍정 자세 aws docker kubernetes argocd jenkins csr ssr webview inapp']\n",
      "Column: Welfare\n",
      "Sample Texts:\n",
      "['', '일해 자율 책임 문화 토스 구성 조직 정보 공유 수평 문화 구성원 위임 신뢰 바탕 세상 변화 효율 방식 가장 영향 집중 획일 프로세스 데이터 사고 적극 토론 가장 효율 문제 해결 위해 협업 실행 동료 분야 최고 수준 인재 자율 책임 원칙 아래 발휘 자율 효율 근무 환경 자율 출퇴근 제도 자율 휴가 재택근무 성장 포괄 임금 운영 관련 비용 최고급 장비 소프트웨어 기별 성과급 매월 통신비 체력 명절 상여금 생일 축하 직장 단체 보험 가족 포함 사비 인재 추천 만원 회사 법인 카드 전원 점심 저녁 식사 비용 야간 교통비 사내 카페 커피 무료 이용 사내 편의점 헤어 살롱 복지 시설 무료 이용 early friday off week silo', '근무 형태 개월 계약 기간 평가 결과 정규직 전환 계약 기간 서로 서로 준비 생각 때문 처우 차등 계약 계약 기간 부서장 요청 기간 조정 근무시간 월요일 금요일 근무 유연근무제 적용 근무 부서 상황 진행 복리 후생 최고 효율 용품 최신 사양 장비 기본 모니터 장비 사내 카페 운영 사내 카페 직원 할인 이용 이동 자리 설치 개인 태블릿 음료 주문 도서 구입 교육비 경조사 화환 휴가 기념일 특별 회식비 음료 식비 야근 저녁 식대 교통비 걱정 야근 택시 다이슨 에어 레쉬 여직원 전용 휴게실 macbook gift', '자율 출퇴근 점심 연차 운동 최고급 장비 밀러 의자 아이맥 무제한 간식 교통 공덕역 거리 교육비 이자 전세 자금 대출', '개발 환경 개발 환경 배포 환경 환경 시차 출퇴근 제로 사이 출근 연차 차시 눈치 보지 사용 자율 복장 출근 신규 입사 웰컴 키트 물품 협업 사용 장비 맥북 프로 듀얼 모니터 소프트웨어 라운지 이용 커피 공유 오피스 근무 분기 타운 미팅 통해 주요 이슈 사항 경영 현황 공유 혜택 복지 협의 통해 현금 복지 카드 회사 단체 보험 가입 실손 보험료 건강검진 유급 생일 조기 퇴근 제도 운영 도서 교육비 매월 동호회 활동 조기 퇴근 명절 생일 선물 nextjsapprouter pagerouter typescript react reactquery html css jenkins argocd cloudfront notion google workspace']\n",
      "Column: Skill\n",
      "Sample Texts:\n",
      "['', '', '', 'java kotlin sql tcp', '']\n",
      "Column: Tag\n",
      "Sample Texts:\n",
      "['출산휴가 육아휴직 무제한 연차 상위 인원 설립', '통신비 보너스 커피 스낵바 재택근무 편의 시설 단체 보험 상위 인원 설립', '설립 상위 유연 근무 커피 스낵바 식대 편의 시설 자기계발 장비', '장비 자기계발 커피 스낵바 상위 설립 아시아 태평양 성장 인원', '사내 동호회 휴가 건강검진 커피 스낵바 단체 보험 자기계발 유연 근무 설립 누적 투자 인원 상위']\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    # 해당 칼럼은 처리하지 않음\n",
    "    if column in ['Title', 'Company', 'Career', 'Deadline', 'Location', 'Duty', 'URL']:\n",
    "        continue\n",
    "    if df[column].dtype == 'object':  # 문자열 데이터에 대해서만 처리\n",
    "        # 전처리 및 명사 추출\n",
    "        df[column] = df[column].apply(lambda x: preprocess_text(x, okt))\n",
    "        column_text = df[column].dropna().tolist()  # NaN 값 제거 및 리스트로 변환\n",
    "        \n",
    "        # 텍스트 샘플 출력 (디버깅용)\n",
    "        print(f\"Column: {column}\")\n",
    "        print(\"Sample Texts:\")\n",
    "        print(column_text[:5])  # 상위 5개 텍스트 샘플 출력\n",
    "        \n",
    "        if len(column_text) > 0:  # 데이터가 있는 경우에만 처리\n",
    "            # TF-IDF 기반 중요 단어 추출\n",
    "            important_words = get_important_words(column_text, n=50)\n",
    "            if important_words:  # 중요 단어가 추출된 경우에만 처리\n",
    "                important_words_dict[column] = important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4004eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 칼럼별 중요 단어를 CSV 파일로 저장\n",
    "for column, words in important_words_dict.items():\n",
    "    df_words = pd.DataFrame(words, columns=['Word', 'Score'])\n",
    "    df_words.to_csv(f'{column}_important_words.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd6e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Work\n",
      "0    주요 책임 글로벌 유저 대상 게임 플랫폼 개발 플랫폼 서비스 운영 지속 개선 게임 ...\n",
      "1    합류 토스 레이스 오프라인 결제 시장 디지털 혁신 만들기 위해 매장 운영 문제 해결...\n",
      "2    프레임워크 개발 유지 보수 자사 솔루션 백오피스 비롯 마이크로 사이트 개발 유지 보...\n",
      "3                모바일 게임 마피아 서버 개발자 채팅 모바일 온라인 게임 서버 개발\n",
      "4    제휴 파트너 효과 테크 서비스 개발 성장 중인 트래블 월렛 차세대 외환 결제 백오피...\n",
      "Name: Work, dtype: object\n",
      "Column: Qualification\n",
      "0    필수 자격 요건 학력 전공 무관 개발 보유 유저 서비스 개발 보유 이해 지식 보유 ...\n",
      "1    분과 프레임워크 사용 개발 주도 문제 발견 분석 해결 이용 정적 타입 분석 reac...\n",
      "2    이해 프론트엔드 프레임워크 이해 개발 의사소통 협업 javascripttypescr...\n",
      "3    자료구조 알고리즘 운영체제 네트워크 컴퓨터 학적 지식 언어 지식 설계 개발 포트폴리...\n",
      "4    관련 개발 실력 보유 활용 어플리케이션 평소 사용 사용 커뮤니케이션 react ty...\n",
      "Name: Qualification, dtype: object\n",
      "Column: Addition\n",
      "0    조직 문화 자율 출퇴근 무제한 연차 휴가 스스로 관리 의견 교환 상호 신뢰 근거 수...\n",
      "1    이력서 작성 추천 플랫폼 관련 공통 서비스 공통 라이브러리 구현 운영 해당 진행 이...\n",
      "2    이해 관심 사용 성능 최적화 관심 이해 테스트 자동화 단위 테스트 라이브러리 모듈 ...\n",
      "3    게임 웹앱 온라인 서비스 백엔드 개발 용량 부하 시스템 설계 운영 리눅스 운영 서버...\n",
      "4    배포 클라우드 배포 환경 이해 프론트엔드 개발 방법 이해 개발 디자인 시스템 재사용...\n",
      "Name: Addition, dtype: object\n",
      "Column: Welfare\n",
      "0                                                     \n",
      "1    일해 자율 책임 문화 토스 구성 조직 정보 공유 수평 문화 구성원 위임 신뢰 바탕 ...\n",
      "2    근무 형태 개월 계약 기간 평가 결과 정규직 전환 계약 기간 서로 서로 준비 생각 ...\n",
      "3    자율 출퇴근 점심 연차 운동 최고급 장비 밀러 의자 아이맥 무제한 간식 교통 공덕역...\n",
      "4    개발 환경 개발 환경 배포 환경 환경 시차 출퇴근 제로 사이 출근 연차 차시 눈치 ...\n",
      "Name: Welfare, dtype: object\n",
      "Column: Skill\n",
      "0                       \n",
      "1                       \n",
      "2                       \n",
      "3    java kotlin sql tcp\n",
      "4                       \n",
      "Name: Skill, dtype: object\n",
      "Column: Tag\n",
      "0                            출산휴가 육아휴직 무제한 연차 상위 인원 설립\n",
      "1             통신비 보너스 커피 스낵바 재택근무 편의 시설 단체 보험 상위 인원 설립\n",
      "2                  설립 상위 유연 근무 커피 스낵바 식대 편의 시설 자기계발 장비\n",
      "3                   장비 자기계발 커피 스낵바 상위 설립 아시아 태평양 성장 인원\n",
      "4    사내 동호회 휴가 건강검진 커피 스낵바 단체 보험 자기계발 유연 근무 설립 누적 투...\n",
      "Name: Tag, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 텍스트 확인 (디버깅용)\n",
    "for column in df.columns:\n",
    "    if column in important_words_dict:\n",
    "        print(f\"Column: {column}\")\n",
    "        print(df[column].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a10b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터프레임 저장\n",
    "df.to_csv('data/preprocessed_data_all.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c93c5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afe07f",
   "metadata": {},
   "source": [
    "2) 조건에 따른 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90903fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"data/preprocessed_data_all.csv\", index_col=None, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16788cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트를 파일에서 읽어오기\n",
    "with open('stopwords-ko.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf7ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의 (명사 추출 및 불용어 제거, 영어 포함)\n",
    "def preprocess_text(text, okt):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # 소문자로 변환\n",
    "        text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # 특수 문자 제거\n",
    "        # 명사 추출\n",
    "        nouns = okt.nouns(text)\n",
    "        # 영어 단어 추출\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "        # 불용어 제거 및 단어 필터링\n",
    "        filtered_nouns = [noun for noun in nouns if noun not in stopwords and len(noun) > 1]\n",
    "        filtered_english = [word for word in english_words if word not in stopwords]\n",
    "        # 한국어 명사와 영어 단어를 결합하여 반환\n",
    "        return ' '.join(filtered_nouns + filtered_english)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ebc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 초기화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12146f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 검색어 입력\n",
    "def ask_question(question):\n",
    "    response = input(question + \" \")\n",
    "    return response\n",
    "\n",
    "# 조건에 따른 필터링\n",
    "def filter_jobs_by_criteria(df, location=None, duty=None, career=None):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # 조건에 따라 데이터 필터링\n",
    "    if location and not pd.isna(location):\n",
    "        filtered_df = filtered_df[filtered_df['Location'].str.strip().str.contains(location.strip(), case=False, na=False)]\n",
    "    if duty and not pd.isna(duty):\n",
    "        filtered_df = filtered_df[filtered_df['Duty'].str.strip().str.contains(duty.strip(), case=False, na=False)]\n",
    "    if career and not pd.isna(career):\n",
    "        filtered_df = filtered_df[filtered_df['Career'].str.strip().str.contains(career.strip(), case=False, na=False)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# 유사 채용 공고 추천 함수 (문답에 따른 범위 좁히기)\n",
    "def recommend_interactively(df, okt):\n",
    "    # 사용자의 선택 조건을 순차적으로 입력\n",
    "    location = ask_question(\"선호하는 지역을 말씀해주세요:\")\n",
    "    duty = ask_question(\"선호하는 직무를 말씀해주세요:\")\n",
    "    career = ask_question(\"경력에 대한 조건을 말씀해주세요:\")\n",
    "\n",
    "    # 필터링된 데이터프레임\n",
    "    filtered_df = filter_jobs_by_criteria(df, location, duty, career)\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(\"해당 조건에 맞는 채용 공고가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 사용자 질의를 입력받고 전처리\n",
    "    user_query = ask_question(\"검색하려는 키워드를 입력해주세요:\")\n",
    "    query_preprocessed = preprocess_text(user_query, okt)\n",
    "\n",
    "    # TF-IDF 벡터화\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # 여러 칼럼을 결합하여 벡터화\n",
    "    filtered_df['combined_text'] = (\n",
    "        filtered_df['Title'].fillna('') + ' ' +\n",
    "        filtered_df['Company'].fillna('') + ' ' +\n",
    "        filtered_df['Work'].fillna('') + ' ' +\n",
    "        filtered_df['Qualification'].fillna('') + ' ' +\n",
    "        filtered_df['Addition'].fillna('') + ' ' +\n",
    "        filtered_df['Welfare'].fillna('') + ' ' +\n",
    "        filtered_df['Skill'].fillna('') + ' ' +\n",
    "        filtered_df['Tag'].fillna('') + ' ' +\n",
    "        filtered_df['Deadline'].fillna('') + ' ' +\n",
    "        filtered_df['Location'].fillna('') + ' ' +\n",
    "        filtered_df['Duty'].fillna('')\n",
    "    )\n",
    "\n",
    "    # TfidfVectorizer를 적용\n",
    "    combined_matrix = vectorizer.fit_transform(filtered_df['combined_text'])\n",
    "\n",
    "    # 사용자 질의를 벡터화\n",
    "    query_vector = vectorizer.transform([query_preprocessed])\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity_scores = cosine_similarity(query_vector, combined_matrix).flatten()\n",
    "\n",
    "    # 유사도 순으로 상위 5개의 인덱스 추출\n",
    "    top_indices = similarity_scores.argsort()[-5:][::-1]\n",
    "\n",
    "    # 상위 5개의 회사, 제목, URL을 추천\n",
    "    recommendations = filtered_df[['Title', 'Company', 'URL']].iloc[top_indices]\n",
    "    \n",
    "    print(\"추천 결과:\")\n",
    "    print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31aa7cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선호하는 지역을 말씀해주세요: 서울\n",
      "선호하는 직무를 말씀해주세요: 개발\n",
      "경력에 대한 조건을 말씀해주세요: 신입\n",
      "검색하려는 키워드를 입력해주세요: 데이터 분석\n",
      "추천 결과:\n",
      "                         Title        Company  \\\n",
      "1198                   데이터 분석가           플래티어   \n",
      "374          Data Engineer(신입)     뤼이드(Riiid)   \n",
      "995                   데이터 엔지니어          하이퍼리즘   \n",
      "133                 데이터 사이언티스트           교보문고   \n",
      "871   Marketing Data Scientist  에이비일팔공(AB180)   \n",
      "\n",
      "                                     URL  \n",
      "1198  https://www.wanted.co.kr/wd/177463  \n",
      "374   https://www.wanted.co.kr/wd/232700  \n",
      "995   https://www.wanted.co.kr/wd/229693  \n",
      "133    https://www.wanted.co.kr/wd/84866  \n",
      "871   https://www.wanted.co.kr/wd/236505  \n"
     ]
    }
   ],
   "source": [
    "# 채용 공고 추천 시스템 실행\n",
    "recommend_interactively(df, okt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3431c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23d188d",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
